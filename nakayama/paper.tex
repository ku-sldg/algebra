\RequirePackage[l2tabu,orthodox]{nag}
\documentclass{article}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{minted}

\input{preamble.tex}

\begin{document}

% Mention an attempt with modules
% Formalizing algebra => crypto
% Interesting steps of the proof
% ssreflect still looks to only deal with finite/discrete objects



\title{ 
Formal Commutative Algebra in Coq: \\ Nakayama's Lemma\thanks{Source code for this work is available on the following site: \url{https://github.com/ku-sldg/algebra}}
}


\date{}

%\author{}
\author{ 
	
	Authors 
	
	\vspace{.2cm}
		
	\\ 
	
	Institute for Information Sciences \\
	The University of Kansas \\
	Lawrence, KS 66045}
%\email{}

%\thanks{Witt acknowledges support from NSF CAREER Award DMS-1945611 and the 2022-23 Ruth I.\, Michler Memorial Prize from the Association for Women in Mathematics.}



\maketitle


\begin{abstract}
\emily[inline]{	Abstract here }
\end{abstract}

\noindent \textbf{Keywords:}
Formalization of Mathematics,
Formal Proof,
Commutative Algebra,
Commutative Ring,
Ideal,
Module over a Ring. 


\section{Introduction}

The mathematical field of \emph{commutative algebra} stems from the study of solutions to polynomial equations.  Research in the field now centers around \emph{commutative rings}--rings in which order does not affect multiplication, i.e., $x \cdot y = y \cdot x$ for any ring elements $x$ and $y$--and fundamental algebraic objects associated to them:  \emph{ideals} of these rings, and \emph{modules} over them. 
Commutative algebra has deep connections with other areas of theoretical mathematics, including number theory and algebraic geometry. 

Commutative algebra also has broad applications to science and technology.  For instance, it has been integral to advances in robotics \cite{cox-little-oshea}, and has helped form our current understanding of the human genome \cite{genetic-algebra}. 
The commutative-algebraic notion of a Gr\"obner basis, a special type of generating set for an ideal in a ring of polynomials, has become a fundamental computational tool in 
coding theory and cryptography (e.g., see  \cite{grobner-bases-cryptography}).
A implementation of Buchberger's algorithm \cite{buchberger} for determining 
Gr\"obner bases of ideals in polynomial rings has been proved correct within the proof assistant Coq \cite{the_coq_development_team_2019_3476303,thery-buchberger}, and an integrated formal development of the algorithm in Coq has also been carried out \cite{persson2001integrated} (see also \cite{grobner-type-theory}). 

Our goal is to newly formalize theoretical, rather than computational, commutative algebra in Coq.  We formally prove \emph{Nakayama's lemma} \cite{nakayama-1951, azumaya}, a fundamental theorem in the field. 
In the process of doing so,  we formalize certain algebraic structures  essential to higher-level algebra, including \emph{local rings} and \emph{modules over commutative rings}. 

The notion of a module over a ring is an extension of the linear-algebraic notion of a vector space over a field, ubiquitous in mathematics and its applications. 
Less frequently referred to as the \emph{Krull-Azumaya theorem}\footnote{Hideyuki Matsumura explains in his text \emph{Commutative Algebra} \cite{matsumura}:  ``{This simple but important lemma is due to T.\ Nakayama, G.\ Azumaya
and W.\ Krull. Priority is obscure, and although it is usually called the Lemma of Nakayama, late Prof.\  Nakayama did not like the name.''}}\,\cite{nagata}, Nakayama's lemma 
describes one way that a finitely generated module over an arbitrary commutative ring acts like a vector space over a field.  
True to the convention that ``lemma'' often refers to a result serving as a stepping stone toward another goal, Nakayama's lemma is applied widely throughout the field, and the result is typically introduced in a first graduate course in commutative algebra \cite{atiyah-macdonald, matsumura, eisenbud}.   

%It is widely applied throughout algebra. %, true to .  However, Nakayama's lemma 
%is also an important result in its own right, and can be thought of as one of the fundamental theorems of 
%commutative algebra, not unlike the fundamental theorems of calculus describing the inverse relationship between derivatives and integrals or the fundamental theorem of arithmetic that makes precise the notion of unique factorization. 
%, or the fundamental theorem of algebra (or \emph{d'Alembert's theorem}) on the roots of complex polynomials.  





\section{Mathematical Basis and Motivation}

\subsection{The Fundamental Algebraic Objects}

\emily[inline]{Something about building up a lot}

\paragraph{Commutative rings.}
In abstract algebra, the quintessential example of a commutative ring is the set of integers
\[\mathbb{Z} = \{ \ldots, -3, -2, -1, 0, 1, 2, 3, \ldots \}.\]
using the natural definitions of addition and multiplication.  

Adding two integers produces another, and the associative and commutative laws hold for addition.  The integers form an \emph{abelian group} under addition since $0 \in \mathbb{Z}$ is the \emph{additive identity} in the sense that adding zero has no effect on any integer, and given any integer $n$, the integer $-n$ is its \emph{additive inverse} in the sense that the sum of $n$ and $-n$ is the additive identity $0$. 

The set of integers also forms a \emph{ring} because it is closed under multiplication, which again satisfies associativity, and the distributive law governing the compatibility of addition and multiplication holds. 
Even more, the integers form a 
\emph{commutative ring} since $n \cdot m = m \cdot n$ for all integers $n$ and $m$.  Moreover,  the integer $1$ is the ring's \emph{multiplicative identity}, since for any  $n \in \mathbb{Z}$ one has $n \cdot 1 = 1 \cdot n = n$, $\mathbb{Z}$ is, furthermore, a \emph{commutative ring with identity}. 

In general, a \emph{ring} is a set $R$ with two binary operations, which we call \emph{addition} and \emph{mutiplication}, typically denoted $\cdot$ and $+$, respectively.  As motivated by the properties of the ring of integers, addition, $R$ must be an abelian group, multiplication must be associative, and the distributive law must hold, i.e., for all $x, y, z \in R$, $(x+y)\cdot z = x \cdot z + y \cdot z$ and $x \cdot (y+z) = x \cdot y + x \cdot z$.   



\emily[inline]{Quickly mention familiar examples}

We will use $0$ to refer to an additive identity, and $1$ will represent a multiplicative identity.

Rather than call upon the \emph{Mathematical Components Library} 
 for Coq \cite{mathcomp}

\paragraph{Ideals.}
The concept of an {ideal} of a ring can be thought of as an extension of the notion of an integer $x$ in the ring of integers $\mathbb{Z}$. 
An \emph{ideal} of commutative ring $R$ is a subset $I$ of $R$ that is itself 
an abelian group under addition,  
which satisfies the following ``absorption'' property: Given any element $a$ of $I$, 
the product  $x \cdot a$ is again in $I$ for any ring element $x \in R$. 

One can verify that given any integer $n$, the set $n\mathbb{Z}$ of its multiples forms an ideal of $\mathbb{Z}$.  For instance, $2 \mathbb{Z}$ consists of all even numbers, and is an abelian group under addition: the sum of two even numbers is even, the additive identity $0$ is eve, and the negative of an even number is even.
Moreover, the absorption property holds since the product of any integer and an even number is again even. 
In fact, every ideal of the ring of integers has this form $n\mathbb{Z}$ for some integer $n$, though ideals in general commutative rings can have more complicated properties. 

Since every integer $n$ can be written as $1 \cdot n$, the ideal $1 \mathbb{Z}$ is the entire ring $\mathbb{Z}$.   One can see that given a commutative ring $R$ itself satisfies the axioms required to be an ideal of $R$.  We call an ideal $I$ of $R$ \emph{proper} if it is strictly contained in $R$.  Every commutative ring with unity contains the proper ideal consisting solely of the additive identity $0$. 

A \emph{maximal ideal} of a commutative ring is a proper ideal that is maximal with respect to inclusion, i.e., no other proper ideal strictly contains it. 
Going back to the ring of integers, $6 \mathbb{Z} \subsetneq 2 \mathbb{Z}$ since every multiple of $6$ is even, 
so $6 \mathbb{Z}$ is not a maximal ideal of $\mathbb{Z}$. 
However, no proper ideal $I$ contains $2 \mathbb{Z}$:  If $2 \mathbb{Z}\subsetneq I \subsetneq \mathbb{Z}$, then $I$  would necessarily contain an odd number $n$.  Writing $n=2k+1$ for some integer $k$, we notice that since $-2k = 2 \cdot -k$  is in $2\mathbb{Z}$, it is also an element of the larger set $I$, and since $I$ is an abelian group under addition, $(2k+1) + $

In fact, $3 \mathbb{Z}$


\subsection{Nakayama's Lemma, Informally}

\begin{nak*}
Let $R$ be a commutative local ring with maximal ideal $\mathfrak{m}$, and suppose that $M$ is a finitely generated $R$-module.  If $M = \mathfrak{m} M$, then $M = 0$.
\end{nak*}


\emily[inline]{Add description of alternate statements of Nakayama.}


\section{Formalization}

\subsection{Our Algebraic Hierarchy}

We started by defining a semigroup class, which declares a binary operation to
be associative. From here, we build up through monoids, which introduce
identities, to groups, which introduce inverses. Note the double equals in
these definitions is notation for an arbitrary equivalence relation over the
group's carrier set which acts as equality.
% \begin{verbatim}
\begin{minted}{coq}
  Infix "==" := equiv (at level 60, no associativity).
  Class Semigroup := {
    semigroup_assoc:
      forall (a b c: Carrier),
        a <o> b <o> c == a <o> (b <o> c);
  }.
  Class Monoid := {
    monoid_semigroup :> Semigroup equiv op;
    monoid_ident_l:
      forall (a: Carrier), ident <o> a == a;
    monoid_ident_r:
      forall (a: Carrier), a <o> ident == a;
  }.
  Class Group := {
    group_monoid :> Monoid equiv op ident;
    group_inv_l:
      forall (a: Carrier), inv a <o> a == ident;
    group_inv_r:
      forall (a: Carrier), a <o> inv a == ident;
  }.
\end{minted}
% \end{verbatim}
The lines like \verb|monoid_semigroup :> Semigroup equiv op;| simply coerce
the monoid typeclass into a semigroup.

While we found later on that we did not need quotients, it is worth remarking
that we got quotients working rather nicely in Coq using typeclasses. An
algebraic quotient are an equivalence relation on the algebraic structure which
preserve that structure. For example, quotient groups are formed by taking a
subgroup and making every element of that subgroup equivalent to the identity.
With \texttt{P} being the predicate for the subgroup, there are two ways to
make an equivalence relation from this description.
% \begin{verbatim}
\begin{minted}{coq}
  Definition left_congru (a b: Carrier) :=
    P (inv a <o> b).
  Definition right_congru (a b: Carrier) :=
    P (a <o> inv b).
\end{minted}
% \end{verbatim}
When these two relations coincide, then we can prove that the equivalence
relation(s) actually preserve the group structure. Subgroups which have this
property are called \emph{normal subgroups}.
% \begin{verbatim}
\begin{minted}{coq}
  Let normal_subgroup_congru_coincide :=
    forall (a b: Carrier),
      left_congru op inv P a b <->
      right_congru op inv P a b.

  Theorem quotient_normal_subgroup_group:
    normal_subgroup_congru_coincide ->
    Group (left_congru op inv P) op ident inv.
\end{minted}
% \end{verbatim}

It is because of quotients that we used equivalence relations to define the
components of group structure. If we were to use the regular Leibniz equality,
this would make it very difficult to say that a quotient group is another
group. But by having the definition of a group depend upon an arbitrary
equivalence relation, we enable our theory to state that a quotient group is
simply a group with under a different equivalence. We didn't loose much as we
could still rely upon Coq's setoid rewrite tactics. Setoids are types equipped
with an equivalence relation.

Moving onwards, we then defined structures for rings, which have two binary
operations: a commutative plus \(+\) and a non-commutative times \(\cdot\), and
structures for commutative rings, where multiplication is commutative and has
an identity. Here we define \emph{ideals} which are normal subgroups under
addition and are absorbing with regards to multiplication, i.e. \(r a\) is in
the ideal whenever \(a\) is in the ideal and \(r\) is any element of the ring.
Again, we defined quotient rings but found them unnecessary in the end. Note
that should an ideal \(I\) contain a unit, an element \(x\) with a
multiplicative inverse \(x^{-1}\), then \(I\) would be the entire ring by the
absorbing property: given any element \(a\) of the ring, then \(a x^{-1}\) is
also in the ring, and \(a x^{-1} x = a 1 = a\in I\). Maximal ideals were
defined next. These are ideals that are proper subsets of the ring while having
no larger ideal except for the ring itself. Below is the definition in Coq,
which uses \texttt{P} as the predicate for the ideal.
% \begin{verbatim}
\begin{minted}{coq}
  Definition maximal_ideal :=
    exists (r: Carrier), (not (P r) /\
      forall (Q: Carrier -> Prop)
          (Q_proper: Proper (equiv ==> iff) Q)
          (Q_ideal: Ideal add zero minus mul Q),
        (forall (r: Carrier), P r -> Q r) ->
        (forall (r: Carrier), Q r) \/
          (forall (r: Carrier), Q r -> P r)).
\end{minted}
% \end{verbatim}
Almost by definition, maximal ideals can contain no units, otherwise they would
fail to be a proper subset of the ring. We could then define a local ring,
which is a ring with a single maximal ideal.
% \begin{verbatim}
\begin{minted}{coq}
  Definition local_ring :=
    exists (P: Carrier -> Prop)
        (P_proper: Proper (equiv ==> iff) P)
        (P_ideal: Ideal add zero minus mul P),
      maximal_ideal P /\
      (forall (Q: Carrier -> Prop)
          (Q_proper: Proper (equiv ==> iff) Q)
          (Q_ideal: Ideal add zero minus mul Q),
        maximal_ideal Q -> forall (r: Carrier), P r <-> Q r).
\end{minted}
% \end{verbatim}

Here we had to include an axiom that in commutative ring, any non-unit \(x\) is
contained in some maximal ideal. This was made into an axiom as the standard
mathematical argument is a proof with potentially infinitely many steps. The
standard argument goes as follows.
\begin{quote}
    Set \(I_{1}\coloneqq (x)\) to be the principal ideal for \(x\), i.e. the
    ideal generated by the single element \(x\). If \(I_{1}\) is not maximal,
    then there exists a strictly larger ideal \(I_{2}\), i.e.
    \(x\in I_{1}\subsetneq I_{2}\). If \(I_{2}\) is not maximal, then there
    exists another strictly larger ideal \(I_{3}\). Continue these arguments
    infinitely many times if necessary in order to get an infinite chain of
    ideals containing \(x\).
    \[x\in I_{1}\subsetneq I_{2}\subsetneq I_{3}\subsetneq\cdots\subsetneq R\]
    It is a simple matter to show that \(\bigcup_{k=1}^{\infty} I_{k}\) is also
    an ideal containing in \(x\). So by Zorn's lemma, there exists a maximal
    ideal in \(R\) which contains \(x\).
\end{quote}
Zorn's lemma is equivalent to the axiom of choice. So we had to add at least
one axiom here in order to proceed. By adding the axiom that we did, not only
will we encapsulate an infinite argument, but we will also avoid the need for
the axiom of choice.

Also, we used classical logic to prove that \(1 - x\) is a unit where \(x\) is
a non-unit in any local ring. The proof used the rule \(\neg\neg P\rightarrow
P\) in order to do a proof by contradiction.

The next structure defined were modules over rings which generalize vector
spaces over fields. We needed to capture the notion of linear combinations of
coefficients and module vectors. This was done by dependently typed vectors,
i.e. lists parameterized by their length. Because there is an overload of the
term ``vector'', we will use that term to refer to module vectors, and use the
term ``list'' to mean length parameterized lists. As we don't use the simpler
kind of lists, this avoids any name collisions. A finitely generated module is
like the vector space \(\mathbf{R}^{n}\) in that there are finitely many
vectors which can generate all other vectors, for \(\mathbf{R}^{n}\) one such
collection of generators are \(\mathbf{e}_{1} = \begin{pmatrix} 1 & 0 & 0 &
  \cdots & 0\end{pmatrix}^{T}\), \(\mathbf{e}_{2} = \begin{pmatrix} 0 & 1 & 0 &
  \cdots & 0\end{pmatrix}^{T}\), \textellipsis, \(\mathbf{e}_{n} =
  \begin{pmatrix} 0 & 0 & 0 & \cdots & 1\end{pmatrix}^{T}\). In our code,
\texttt{M} is the type of module elements, \texttt{R} is the type of ring
elements which act as coefficients, and \texttt{t A n} is a list whose elements
are of type \texttt{A} and whose length is \texttt{n}.
% \begin{verbatim}
\begin{minted}{coq}
  Definition finitely_generated {n: nat}(basis: t M n) :=
    forall (vector: M),
      exists (coeffs: t R n),
        vector =M= linear_combin coeffs basis.
\end{minted}
% \end{verbatim}

Next, we defined the product of an ideal \(I\) and a module \(M\), denoted as
\(I M\), which is defined to be all linear combinations of vectors from \(M\)
and coefficients from \(I\). This can be easily shown to be a submodule of
\(M\). We represented this in Coq as a predicate over \(M\).
% \begin{verbatim}
\begin{minted}{coq}
  Context (P: R -> Prop).
  Context {P_proper: Proper (Requiv ==> iff) P}.
  Context {P_ideal: Ideal Radd Rzero Rminus Rmul P}.
  
  Definition ideal_module (x: M): Prop :=
    exists (n: nat)(coeffs: t R n)(vectors: t M n),
      Forall P coeffs /\
      x =M= linear_combin Madd Mzero action coeffs vectors.
\end{minted}
% \end{verbatim}
The \verb|Forall P coeffs| ensures that every element of the coefficient list
\texttt{coeffs} satisfies the predicate \texttt{P}. From here, we can move to
stating Nakayama's lemma.

\section{Formally Proving Nakayama's Lemma}


\begin{theorem}[Nakayama's lemma]
  Let \(M\) be a finitely generated module over a local ring \(R\) with maximal
  ideal \(\mathfrak{m}\). If \(\mathfrak{m} M = M\), then \(M = 0\), which is
  to say that \(M\) is the zero module which consists of exactly one element,
  that being the zero vector.
\end{theorem}

We needed a lemma before moving on to prove the main theorem which states that
for a finitely generated module \(M\) with a basis
\(b_{1}, b_{2}, \dots, b_{n}\), any element \(x\in I M\) where \(I\) is an
ideal can be written as a linear combination of the basis vectors with
coefficients coming from \(I\). The proof of this lemma is straightforward and
follows the standard, informal mathematical argument which inductively goes
through the vectors needed to generate \(x\) and shows that each vector can be
rewritten in terms of the basis vectors times elements of the ideal \(I\) as
follows.
\begin{align*}
  x & = \sum_{k=1}^{m} u_{k} a_{k} \\
    & = \sum_{k=1}^{m} u_{k} (r_{k1} b_{1} + \cdots + r_{kn} b_{n}) \\
    & = \sum_{j=1}^{n} (u_{1} r_{1j} + \cdots + u_{m} r_{mj}) b_{j}
\end{align*}
Since each \(u_{k}\) comes from the ideal \(I\), the absorbing property of
ideals guarantees that \(u_{k} r_{k j}\) is also contained in \(I\). As ideals
are also closed under addition, it follows that \(u_{1} r_{1j} + \cdots +
u_{m} r_{m j}\) are all elements of \(I\). Thus, \(x\) can be written as a
linear combination of the basis vectors with the coefficients coming from
\(I\).

For the proof of Nakayama's lemma, it too follows the standard, informal
mathematical argument. Do induction on the number of vectors needed to generate
the module \(M\). The base case where \(M\) is generated by 0 vectors is by
definition true since an empty linear combination is conventionally taken to be
the zero vector. The inductive case where \(M\) is generated by the vectors
\(b_{1}, \dots, b_{m}, b_{m+1}\). By assumption \(M = \mathfrak{m} M\) with
\(b_{1}\in M\). Then \(b_{1}\in\mathfrak{m} M\) meaning that by our lemma,
there exists a linear combination for \(b_{1}\), say
\[b_{1} = u_{1} b_{1} + \cdots + u_{m+1} b_{m+1}\]
for some \(u_{1}, \dots, u_{m+1}\in\mathfrak{m}\). Collect the \(b_{1}\) terms
on the left-hand side of the equation to get that
\[(1 - u_{1}) b_{1} = u_{2} b_{2} + \cdots + u_{m+1} b_{m+1}\text{.}\]
As mentioned when defining maximal ideals, \(\mathfrak{m}\) can only contain
non-units. Namely, \(u_{1}\) must be a non-unit. Then we had already proven
that \(1 - u_{1}\) is a unit as we are in a local ring. Let \(v_{1}\) be the
multiplicative inverse of \(1 - u_{1}\), and multiply it on both sides of the
equation.
\begin{align*}
  v_{1} (1 - u_{1}) b_{1}
    & = v_{1} u_{2} b_{2} + \cdots + v_{1} u_{m+1} b_{m+1} \\
  1 b_{1} & = \\
  b_{1} & = v_{1} u_{2} b_{2} + \cdots + v_{1} u_{m+1} b_{m+1} \\
\end{align*}
We have thus found that one of the basis vectors is not needed to generate this
module. Using the induction hypothesis, we have that \(M = 0\). Showing that
the induction hypothesis holds in Coq takes more work than in an informal
proof, but this extra work is just a lot of bookkeeping.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
